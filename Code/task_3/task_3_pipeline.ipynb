{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "586991f0",
   "metadata": {},
   "source": [
    "# Bengaluru Last Mile: Journey Time Prediction Pipeline\n",
    "\n",
    "This notebook implements the full prediction pipeline for the \"Bengaluru Last Mile Challenge\". It takes journey details as input and predicts the time for 8 distinct segments of a multi-modal trip involving two auto-rickshaw rides and one bus journey.\n",
    "\n",
    "**Note:** This notebook is designed for **inference**. It loads pre-processed data and pre-trained models. The original data cleaning, feature engineering, and model training steps were performed separately. Placeholder sections are included below to indicate where those steps would fit in a complete end-to-end workflow.\n",
    "\n",
    "**Structure:**\n",
    "1.  **Setup:** Importing libraries and configuring paths.\n",
    "2.  **Helper Functions:** Core utilities for geo-processing and data handling.\n",
    "3.  **Predictor Classes:** The main `AutoTimePredictor` and `BusETAPredictor` classes.\n",
    "4.  **(Placeholder) Data Preprocessing:** Description of necessary preprocessing steps.\n",
    "5.  **(Placeholder) Model Training:** Description of the model training process.\n",
    "6.  **Inference Pipeline:** The main function to process journeys.\n",
    "7.  **Execution:** Running the pipeline on sample input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224ef553",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import argparse\n",
    "import warnings\n",
    "from datetime import timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import pickle\n",
    "import h3\n",
    "import networkx as nx\n",
    "import osmnx as ox\n",
    "from geopy.distance import geodesic\n",
    "from shapely.geometry import Point, Polygon, box\n",
    "from shapely.ops import transform\n",
    "from pyproj import CRS, Transformer\n",
    "import math\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# ---------------------------\n",
    "# Path and Config Setup\n",
    "# ---------------------------\n",
    "print(\"Setting up paths and constants...\")\n",
    "IN_DOCKER = os.path.exists(\"/app\")\n",
    "BASE_PATH = \"/app\" if IN_DOCKER else \".\"\n",
    "REF_PATH = os.path.join(BASE_PATH, \"ref_data\")\n",
    "\n",
    "# --- Task 1 (Bus) Paths ---\n",
    "STOPS_PATH = os.path.join(REF_PATH, \"stops_clean.csv\")\n",
    "ROUTE_SEQ_PATH = os.path.join(REF_PATH, \"route_to_stop_clean.csv\")\n",
    "MODEL_PATH = os.path.join(REF_PATH, \"models/eta_model_hypertuned.pkl\")\n",
    "ENCODERS_PATH = os.path.join(REF_PATH, \"models/encoders_hypertuned.pkl\")\n",
    "SPEED_PATH = os.path.join(REF_PATH, \"models/route_avg_speed_m_s_hypertuned.pkl\")\n",
    "\n",
    "# --- Task 2 (Auto) Paths ---\n",
    "SDR_PATH = os.path.join(REF_PATH, \"processed_hex_avg_SDR.parquet\")\n",
    "AUTO_SPEED_PATH = os.path.join(REF_PATH, \"smoothed_speed_full.parquet\")\n",
    "GRAPH_PATH = os.path.join(REF_PATH, \"bangalore_graph_99.6.graphml\")\n",
    "\n",
    "# --- Task 2 Constants ---\n",
    "H3_RES = 7\n",
    "CIRCLE_RADIUS_M = 1840.0\n",
    "PA_ALPHA, PA_BETA = 2.0, 0.0\n",
    "PB_ALPHA, PB_BETA = 0.2, 0\n",
    "PC_CONST = 10.0 # Fallback trip duration in minutes\n",
    "SDR_FAILURE_THRESHOLD = 0.1\n",
    "DEFAULT_AUTO_SPEED_KPH = 19.489212994167335\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadb7a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Helper Functions (Consolidated and made robust)\n",
    "# ---------------------------\n",
    "\n",
    "def parse_coordinates(s):\n",
    "    if pd.isna(s): return (None, None)\n",
    "    try:\n",
    "        parts = [float(p.strip()) for p in str(s).strip().replace(\"(\", \"\").replace(\")\", \"\").replace('\"', '').split(\",\")]\n",
    "        return (parts[0], parts[1]) if len(parts) == 2 else (None, None)\n",
    "    except:\n",
    "        return (None, None)\n",
    "\n",
    "def safe_read_parquet(path):\n",
    "    if not os.path.exists(path):\n",
    "        alt_path = os.path.join(\"/app/data\", os.path.basename(path)) # Adjusted for notebook context\n",
    "        if os.path.exists(alt_path):\n",
    "            return pd.read_parquet(alt_path)\n",
    "        if IN_DOCKER and not os.path.isabs(path):\n",
    "            alt_path_2 = os.path.join(\"/app\", path)\n",
    "            if os.path.exists(alt_path_2):\n",
    "                return pd.read_parquet(alt_path_2)\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "def build_aeqd_transformer(lat_center, lon_center):\n",
    "    aeqd_proj4 = f\"+proj=aeqd +lat_0={lat_center} +lon_0={lon_center} +units=m +datum=WGS84 +no_defs\"\n",
    "    aeqd_crs = CRS.from_proj4(aeqd_proj4)\n",
    "    wgs84 = CRS.from_epsg(4326)\n",
    "    to_proj = Transformer.from_crs(wgs84, aeqd_crs, always_xy=True)\n",
    "    return to_proj.transform, None\n",
    "\n",
    "def get_h3_index(lat, lon, res):\n",
    "    \"\"\"Robustly gets the H3 index as a HEX STRING.\"\"\"\n",
    "    try:\n",
    "        return h3.latlng_to_cell(lat, lon, res)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def convert_h3_to_int64(h3_hex_str):\n",
    "    \"\"\"Converts a hexadecimal H3 string to a 64-bit integer, safely handling invalid inputs.\"\"\"\n",
    "    if pd.isna(h3_hex_str) or h3_hex_str is None:\n",
    "        return np.int64(0)\n",
    "    try:\n",
    "        return np.int64(int(str(h3_hex_str), 16))\n",
    "    except ValueError:\n",
    "        return np.int64(0)\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4f1976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Task 2: Auto Time Predictor Class\n",
    "# ---------------------------\n",
    "class AutoTimePredictor:\n",
    "    def __init__(self):\n",
    "        print(\"[Auto] Initializing AutoTimePredictor...\")\n",
    "        \n",
    "        # --- SDR Data Setup ---\n",
    "        sdr_df = safe_read_parquet(SDR_PATH)\n",
    "        sdr_long_df = sdr_df.melt(id_vars=['h3_index'], value_vars=[f'slot_{i}' for i in range(96)], var_name='slot_name', value_name='SDR')\n",
    "        sdr_long_df['slot'] = sdr_long_df['slot_name'].str.split('_').str[-1].astype(int)\n",
    "        sdr_long_df['h3_index'] = sdr_long_df['h3_index'].astype(np.int64)\n",
    "        self.sdr_series = sdr_long_df.set_index([\"h3_index\", \"slot\"])[\"SDR\"]\n",
    "        \n",
    "        # --- Speed Data Setup ---\n",
    "        self.speed_df = safe_read_parquet(AUTO_SPEED_PATH)\n",
    "        self.speed_df.columns = [str(col) for col in self.speed_df.columns]\n",
    "        if 'u' in self.speed_df.columns: self.speed_df['u'] = self.speed_df['u'].astype(np.int64)\n",
    "        if 'v' in self.speed_df.columns: self.speed_df['v'] = self.speed_df['v'].astype(np.int64)\n",
    "        self.default_speed_kph = np.float64(DEFAULT_AUTO_SPEED_KPH)\n",
    "        self.G = None\n",
    "        print(\"[Auto] AutoTimePredictor ready.\")\n",
    "\n",
    "    def _get_h3_index(self, lat, lon):\n",
    "        return get_h3_index(lat, lon, H3_RES)\n",
    "\n",
    "    def _compute_hex_circle_weights(self, lat, lon, k_ring_k=3):\n",
    "        center_h3_hexadecimal = self._get_h3_index(lat, lon)\n",
    "        if center_h3_hexadecimal is None or center_h3_hexadecimal == '0':\n",
    "            return []\n",
    "        center_h3_int64 = convert_h3_to_int64(center_h3_hexadecimal)\n",
    "        if center_h3_int64 == np.int64(0):\n",
    "            return []\n",
    "        \n",
    "        candidates = h3.grid_disk(center_h3_hexadecimal, k_ring_k)\n",
    "        to_proj, _ = build_aeqd_transformer(lat, lon)\n",
    "        cx, cy = to_proj(lon, lat)\n",
    "        circle_proj = Point(cx, cy).buffer(CIRCLE_RADIUS_M, resolution=64)\n",
    "\n",
    "        weights = []\n",
    "        for h_hex_str in candidates: \n",
    "            h3_index_int64 = convert_h3_to_int64(h_hex_str)\n",
    "            try:\n",
    "                boundary_func = h3.cell_to_boundary\n",
    "            except AttributeError:\n",
    "                boundary_func = h3.h3_to_geo_boundary\n",
    "            \n",
    "            hex_coords = [(lng, lat) for lat, lng in boundary_func(h_hex_str)] \n",
    "            hex_poly_wgs = Polygon(hex_coords)\n",
    "            hex_poly_proj = transform(lambda x, y: to_proj(x, y), hex_poly_wgs)\n",
    "            inter = hex_poly_proj.intersection(circle_proj)\n",
    "            inter_area = inter.area if not inter.is_empty else 0.0\n",
    "            hex_area = hex_poly_proj.area if hex_poly_proj.area > 0 else 1.0\n",
    "            weight = inter_area / hex_area\n",
    "            if weight > 0:\n",
    "                weights.append((h3_index_int64, weight))\n",
    "        return weights\n",
    "\n",
    "    def _predict_pa_pb(self, lat, lon, req_time_ist):\n",
    "        slot = min(max((req_time_ist.hour * 60 + req_time_ist.minute) // 15, 0), 95)\n",
    "        weights = self._compute_hex_circle_weights(lat, lon)\n",
    "        if not weights:\n",
    "            center_h3_hex = self._get_h3_index(lat, lon)\n",
    "            center_h3_int = convert_h3_to_int64(center_h3_hex)\n",
    "            if center_h3_int != np.int64(0):\n",
    "                weights = [(center_h3_int, 1.0)]\n",
    "            else:\n",
    "                return 30.0, 1.0\n",
    "\n",
    "        SDRs, areas = [], []\n",
    "        for h, w in weights:\n",
    "            SDRs.append(self.sdr_series.get((h, slot), 0.0))\n",
    "            areas.append(w)\n",
    "\n",
    "        weighted_sdr = np.sum(np.array(SDRs) * np.array(areas))\n",
    "        \n",
    "        if weighted_sdr < SDR_FAILURE_THRESHOLD:\n",
    "            pa = 30.0\n",
    "        else:\n",
    "            y_pa = PA_ALPHA * math.log(1 + weighted_sdr) + PA_BETA\n",
    "            pa = 9.0 * (1 - (1 / (1 + np.exp(-y_pa))))\n",
    "        \n",
    "        y_pb = PB_ALPHA * weighted_sdr + PB_BETA\n",
    "        pb = 7 * (1 - (1 / (1 + np.exp(-y_pb))))\n",
    "        return pa, pb\n",
    "        \n",
    "    def _edge_speed_in_df(self, u, v, timeslot_col, G):\n",
    "        def mean_speed(df):\n",
    "            return float(df[timeslot_col].mean()) if not df.empty and df[timeslot_col].mean() > 0.5 else np.nan\n",
    "\n",
    "        match = self.speed_df[((self.speed_df['u'] == u) & (self.speed_df['v'] == v)) | ((self.speed_df['u'] == v) & (self.speed_df['v'] == u))]\n",
    "        speed = mean_speed(match)\n",
    "        if not pd.isna(speed): return speed\n",
    "        return np.nan\n",
    "\n",
    "    def _predict_pc(self, start_loc, end_loc, req_time_ist):\n",
    "        try:\n",
    "            seconds_from_midnight = req_time_ist.hour * 3600 + req_time_ist.minute * 60 + req_time_ist.second\n",
    "            timeslot = str(seconds_from_midnight // 900)\n",
    "            \n",
    "            buffer_deg = 0.01\n",
    "            north, south = max(start_loc[0], end_loc[0]) + buffer_deg, min(start_loc[0], end_loc[0]) - buffer_deg\n",
    "            east, west = max(start_loc[1], end_loc[1]) + buffer_deg, min(start_loc[1], end_loc[1]) - buffer_deg\n",
    "            \n",
    "            G = ox.graph_from_polygon(box(west, south, east, north), network_type=\"drive_service\", simplify=True)\n",
    "            u_node = ox.nearest_nodes(G, start_loc[1], start_loc[0])\n",
    "            v_node = ox.nearest_nodes(G, end_loc[1], end_loc[0])\n",
    "            \n",
    "            route_nodes = nx.shortest_path(G, source=u_node, target=v_node, weight='length')\n",
    "            \n",
    "            total_time_sec = 0\n",
    "            last_speed = self.default_speed_kph\n",
    "            for i in range(len(route_nodes) - 1):\n",
    "                u, v = route_nodes[i], route_nodes[i + 1]\n",
    "                avg_speed = self._edge_speed_in_df(u, v, timeslot, G)\n",
    "                avg_speed = last_speed if pd.isna(avg_speed) else avg_speed\n",
    "                last_speed = avg_speed\n",
    "                \n",
    "                edge_length_m = G.get_edge_data(u, v, key=0).get('length', 0)\n",
    "                if avg_speed > 0:\n",
    "                    total_time_sec += (edge_length_m / 1000) / avg_speed * 3600\n",
    "            return total_time_sec / 60\n",
    "        except Exception as e:\n",
    "            print(f\" -> CHECKPOINT: Auto PC dynamic route calculation failed. Using fallback. Error: {e}\")\n",
    "            return PC_CONST\n",
    "\n",
    "    def predict_trip(self, start_loc, end_loc, req_time_ist):\n",
    "        pa, pb = self._predict_pa_pb(start_loc[0], start_loc[1], req_time_ist)\n",
    "        pc = self._predict_pc(start_loc, end_loc, req_time_ist)\n",
    "        return pa, pb, pc\n",
    "\n",
    "print(\"AutoTimePredictor class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e48abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Task 1: Bus ETA Predictor Class\n",
    "# ---------------------------\n",
    "class BusETAPredictor:\n",
    "    def __init__(self, default_buffer_time_mins=0.5):\n",
    "        print(\"[Bus] Initializing BusETAPredictor...\")\n",
    "        self.model = joblib.load(MODEL_PATH)\n",
    "        with open(ENCODERS_PATH, \"rb\") as f:\n",
    "            self.encoders = pickle.load(f)\n",
    "        self.route_avg_speed = joblib.load(SPEED_PATH)\n",
    "        self.global_avg_speed = float(pd.Series(self.route_avg_speed).mean())\n",
    "        \n",
    "        self.df_stops = pd.read_csv(STOPS_PATH)\n",
    "        self.df_route_seq = pd.read_csv(ROUTE_SEQ_PATH)\n",
    "        self.df_route_seq['route_id'] = self.df_route_seq['route_id'].astype(str)\n",
    "\n",
    "        self.default_buffer_time_mins = default_buffer_time_mins\n",
    "        print(f\"[Bus] Dwell time configured: Using a uniform buffer of {self.default_buffer_time_mins} mins for all stops.\")\n",
    "        print(\"[Bus] BusETAPredictor ready.\")\n",
    "\n",
    "    def get_stop_coords(self, stop_id):\n",
    "        stop_info = self.df_stops[self.df_stops['stop_id'] == stop_id]\n",
    "        if not stop_info.empty:\n",
    "            return (stop_info.iloc[0]['stop_lat'], stop_info.iloc[0]['stop_lon'])\n",
    "        return (None, None)\n",
    "\n",
    "    def _parse_stop_list(self, stop_list_str):\n",
    "        if pd.isna(stop_list_str): return []\n",
    "        return [int(n) for n in re.findall(r\"\\d+\", str(stop_list_str))]\n",
    "\n",
    "    def _predict_segment_duration_mins(self, route_id, from_stop_id, to_stop_id, distance_m, timestamp_utc):\n",
    "        r_enc = self.encoders['route_id'].transform([str(route_id)])[0] if str(route_id) in self.encoders['route_id'].classes_ else 0\n",
    "        fs_enc = self.encoders['from_stop'].transform([str(from_stop_id)])[0] if str(from_stop_id) in self.encoders['from_stop'].classes_ else 0\n",
    "        ts_enc = self.encoders['to_stop'].transform([str(to_stop_id)])[0] if str(to_stop_id) in self.encoders['to_stop'].classes_ else 0\n",
    "        avg_speed = self.route_avg_speed.get(str(route_id), self.global_avg_speed)\n",
    "\n",
    "        features = pd.DataFrame([{\n",
    "            'route_id_enc': r_enc, 'from_stop_enc': fs_enc, 'to_stop_enc': ts_enc,\n",
    "            'distance_m': distance_m, 'avg_speed_m_s': avg_speed,\n",
    "            'start_hour': int(timestamp_utc.hour), 'day_of_week': int(timestamp_utc.dayofweek)\n",
    "        }])\n",
    "        return float(np.expm1(self.model.predict(features)[0]))\n",
    "\n",
    "    def predict_eta_to_stop(self, last_ping, route_id, target_stop_id):\n",
    "        ping_time = pd.to_datetime(last_ping['vehicle_timestamp'], unit='s', utc=True)\n",
    "        ping_loc = (last_ping['latitude'], last_ping['longitude'])\n",
    "        \n",
    "        route_info = self.df_route_seq[self.df_route_seq['route_id'] == str(route_id)]\n",
    "        if route_info.empty: return None\n",
    "        \n",
    "        stop_seq = self._parse_stop_list(route_info['stop_id_list'].iloc[0])\n",
    "        if not stop_seq: return None\n",
    "        \n",
    "        stop_coords = self.df_stops.set_index('stop_id').loc[stop_seq][['stop_lat', 'stop_lon']]\n",
    "        dists = stop_coords.apply(lambda r: geodesic(ping_loc, (r.stop_lat, r.stop_lon)).meters, axis=1)\n",
    "        next_idx = dists.idxmin()\n",
    "        \n",
    "        try:\n",
    "            target_seq_idx = stop_seq.index(target_stop_id)\n",
    "            next_seq_idx = stop_seq.index(next_idx)\n",
    "        except ValueError:\n",
    "            return None\n",
    "\n",
    "        if target_seq_idx < next_seq_idx: return None\n",
    "\n",
    "        cumulative_mins = 0.0\n",
    "        \n",
    "        # From ping to first stop\n",
    "        first_stop_coords = self.get_stop_coords(next_idx)\n",
    "        dist_to_first_stop = geodesic(ping_loc, first_stop_coords).meters\n",
    "        from_stop_id = stop_seq[next_seq_idx-1] if next_seq_idx > 0 else next_idx\n",
    "        cumulative_mins += self._predict_segment_duration_mins(route_id, from_stop_id, next_idx, dist_to_first_stop, ping_time)\n",
    "\n",
    "        if target_stop_id == next_idx:\n",
    "            return ping_time + timedelta(minutes=cumulative_mins)\n",
    "            \n",
    "        # From first stop to subsequent stops\n",
    "        for i in range(next_seq_idx, target_seq_idx):\n",
    "            f_stop, t_stop = stop_seq[i], stop_seq[i+1]\n",
    "            f_loc, t_loc = self.get_stop_coords(f_stop), self.get_stop_coords(t_stop)\n",
    "            dist_m = geodesic(f_loc, t_loc).meters\n",
    "            cumulative_mins += self._predict_segment_duration_mins(route_id, f_stop, t_stop, dist_m, ping_time)\n",
    "            if t_stop != target_stop_id:\n",
    "                cumulative_mins += self.default_buffer_time_mins\n",
    "                \n",
    "        return ping_time + timedelta(minutes=cumulative_mins)\n",
    "\n",
    "    def predict_trip_duration(self, route_id, start_stop_id, end_stop_id, departure_time_utc):\n",
    "        route_info = self.df_route_seq[self.df_route_seq['route_id'] == str(route_id)]\n",
    "        if route_info.empty:\n",
    "            return 15.0 \n",
    "        \n",
    "        stop_seq = self._parse_stop_list(route_info['stop_id_list'].iloc[0])\n",
    "        try:\n",
    "            start_idx = stop_seq.index(start_stop_id)\n",
    "            end_idx = stop_seq.index(end_stop_id)\n",
    "        except ValueError:\n",
    "            return 15.0 \n",
    "\n",
    "        if start_idx >= end_idx:\n",
    "            return 5.0\n",
    "        \n",
    "        total_duration_mins = 0.0\n",
    "        for i in range(start_idx, end_idx):\n",
    "            f_stop, t_stop = stop_seq[i], stop_seq[i+1]\n",
    "            f_loc, t_loc = self.get_stop_coords(f_stop), self.get_stop_coords(t_stop)\n",
    "            dist_m = geodesic(f_loc, t_loc).meters\n",
    "            total_duration_mins += self._predict_segment_duration_mins(route_id, f_stop, t_stop, dist_m, departure_time_utc)\n",
    "            if t_stop != end_stop_id:\n",
    "                total_duration_mins += self.default_buffer_time_mins\n",
    "        return total_duration_mins\n",
    "\n",
    "print(\"BusETAPredictor class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ec6af4",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing\n",
    "\n",
    "This step is **not** performed in this notebook; we load the results of prior preprocessing. A complete pipeline would require the following:\n",
    "\n",
    "* **Bus Stops Data (`stops_clean.csv`):**\n",
    "    * Loading raw bus stop locations.\n",
    "    * Cleaning column names, handling missing values, and ensuring correct data types.\n",
    "* **Route Sequence Data (`route_to_stop_clean.csv`):**\n",
    "    * Processing files that map bus routes to an ordered sequence of stop IDs.\n",
    "    * Consolidating this information into a single clean file.\n",
    "* **SDR Data (`processed_hex_avg_SDR.parquet`):**\n",
    "    * This likely involves processing raw auto-rickshaw trip data (pickups and dropoffs).\n",
    "    * Aggregating trip counts into H3 hexagonal grids at different time slots (e.g., 15-minute intervals) to calculate a Supply-to-Demand Ratio (SDR).\n",
    "* **Auto Speed Data (`smoothed_speed_full.parquet`):**\n",
    "    * Processing GPS pings from auto-rickshaws.\n",
    "    * Mapping these pings to the OSMnx road network graph.\n",
    "    * Calculating the average speed for each road segment (`u`, `v` nodes) for each time slot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac58c88",
   "metadata": {},
   "source": [
    "## 2. Model Training\n",
    "\n",
    "This notebook loads a pre-trained model (`eta_model_hypertuned.pkl`) and encoders. The training process for the bus ETA model would have involved:\n",
    "\n",
    "1.  **Feature Engineering:**\n",
    "    * Creating a dataset where each row represents a bus trip segment between two consecutive stops.\n",
    "    * Calculating the `distance_m` between the stops.\n",
    "    * Calculating the historical average speed (`avg_speed_m_s`) for the entire route.\n",
    "    * Extracting time-based features like `start_hour` and `day_of_week`.\n",
    "    * Categorical features like `route_id`, `from_stop`, and `to_stop` would be identified.\n",
    "2.  **Encoding:**\n",
    "    * Using `LabelEncoder` or a similar technique to convert categorical features into numerical representations. These fitted encoders are saved in `encoders_hypertuned.pkl` to be used during inference.\n",
    "3.  **Model Selection and Training:**\n",
    "    * Choosing a regression model (e.g., LightGBM, XGBoost) to predict the travel time for a segment.\n",
    "    * The target variable would likely be `log1p(travel_time_seconds)` to handle skewed distributions. The prediction is then transformed back using `expm1`.\n",
    "    * Training the model on the engineered dataset.\n",
    "    * Performing hyperparameter tuning to find the best model configuration.\n",
    "4.  **Serialization:**\n",
    "    * Saving the trained model (`.pkl`) and encoders (`.pkl`) for later use in this inference script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4353e862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Main Execution Logic (Evaluation/Inference)\n",
    "# ---------------------------\n",
    "def main(input_csv_path, output_json_path):\n",
    "    print(\"--- Starting Bengaluru Last Mile Challenge - Task 3 ---\")\n",
    "    \n",
    "    # This is a tunable parameter for the bus stop buffer time in minutes.\n",
    "    default_buffer_in_mins = 0.0\n",
    "\n",
    "    auto_predictor = AutoTimePredictor()\n",
    "    bus_predictor = BusETAPredictor(default_buffer_time_mins=default_buffer_in_mins)\n",
    "\n",
    "    df_journeys = pd.read_csv(input_csv_path)\n",
    "    output_results = {}\n",
    "\n",
    "    for _, journey in df_journeys.iterrows():\n",
    "        jid = journey['jid']\n",
    "        print(f\"\\n--- Processing Journey ID: {jid} ---\")\n",
    "        try:\n",
    "            bus_data_path = journey['path_to_the_parquet_file']\n",
    "            df_bus_live = safe_read_parquet(bus_data_path)\n",
    "            \n",
    "            bus_timestamp = pd.to_datetime(df_bus_live['vehicle_timestamp'].iloc[0], unit='s', utc=True)\n",
    "            journey_date_str = bus_timestamp.strftime('%Y-%m-%d')\n",
    "\n",
    "            start_loc = parse_coordinates(journey['auto_1_ride_start_location'])\n",
    "            req_time_ist = pd.to_datetime(f\"{journey_date_str} {journey['auto_1_ride_request_time']}\").tz_localize('Asia/Kolkata')\n",
    "            boarding_stop_id = int(journey['auto_1_ride_end_bus_stop_ID'])\n",
    "            disembark_stop_id = int(journey['bus_trip_end_bus_stop_ID'])\n",
    "            final_dest_loc = parse_coordinates(journey['auto_2_ride_end_location'])\n",
    "\n",
    "            # 1. First Auto Ride\n",
    "            boarding_stop_loc = bus_predictor.get_stop_coords(boarding_stop_id)\n",
    "            a1, a2, a3 = auto_predictor.predict_trip(start_loc, boarding_stop_loc, req_time_ist)\n",
    "\n",
    "            # 2. Arrival at Bus Stop\n",
    "            arrival_at_bus_stop_ist = req_time_ist + timedelta(minutes=(a1 + a2 + a3))\n",
    "            arrival_at_bus_stop_utc = arrival_at_bus_stop_ist.tz_convert('UTC')\n",
    "\n",
    "            # 3. Bus Leg (a4, a5)\n",
    "            route_id = df_bus_live['route_id'].mode()[0]\n",
    "            future_bus_etas = {}\n",
    "            for trip_id in df_bus_live['trip_id'].unique():\n",
    "                last_ping = df_bus_live[df_bus_live['trip_id'] == trip_id].sort_values('vehicle_timestamp').iloc[-1]\n",
    "                eta_utc = bus_predictor.predict_eta_to_stop(last_ping, route_id, boarding_stop_id)\n",
    "                if eta_utc and eta_utc > arrival_at_bus_stop_utc:\n",
    "                    future_bus_etas[trip_id] = eta_utc\n",
    "            \n",
    "            if not future_bus_etas:\n",
    "                actual_boarding_time_utc = arrival_at_bus_stop_utc + timedelta(minutes=15)\n",
    "            else:\n",
    "                _, actual_boarding_time_utc = min(future_bus_etas.items(), key=lambda item: item[1])\n",
    "\n",
    "            a4 = max(0, (actual_boarding_time_utc - arrival_at_bus_stop_utc).total_seconds() / 60)\n",
    "            a5 = bus_predictor.predict_trip_duration(route_id, boarding_stop_id, disembark_stop_id, actual_boarding_time_utc)\n",
    "\n",
    "            # 4. Arrival at Destination Stop\n",
    "            arrival_at_dest_stop_utc = actual_boarding_time_utc + timedelta(minutes=a5)\n",
    "            arrival_at_dest_stop_ist = arrival_at_dest_stop_utc.tz_convert('Asia/Kolkata')\n",
    "\n",
    "            # 5. Second Auto Ride\n",
    "            disembark_stop_loc = bus_predictor.get_stop_coords(disembark_stop_id)\n",
    "            a6, a7, a8 = auto_predictor.predict_trip(disembark_stop_loc, final_dest_loc, arrival_at_dest_stop_ist)\n",
    "            \n",
    "            # --- Store Results ---\n",
    "            final_predictions = {\n",
    "                \"a1\": round(a1, 4), \"a2\": round(a2, 4), \"a3\": round(a3, 4),\n",
    "                \"a4\": round(a4, 4), \"a5\": round(a5, 4), \"a6\": round(a6, 4),\n",
    "                \"a7\": round(a7, 4), \"a8\": round(a8, 4)\n",
    "            }\n",
    "            output_results[str(jid)] = final_predictions\n",
    "            print(f\" -> FINAL PREDICTIONS for Journey {jid}: {final_predictions}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" -> FATAL ERROR processing Journey ID {jid}: {e}\")\n",
    "            output_results[str(jid)] = {\"a1\": 2.0, \"a2\": 3.0, \"a3\": 15.0, \"a4\": 10.0, \"a5\": 20.0, \"a6\": 2.0, \"a7\": 3.0, \"a8\": 15.0}\n",
    "        print(f\"--- Finished Journey ID: {jid} ---\")\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_json_path), exist_ok=True)\n",
    "    with open(output_json_path, 'w') as f:\n",
    "        json.dump(output_results, f, indent=4)\n",
    "    print(f\"\\nâœ… Predictions complete. Output saved to {output_json_path}\")\n",
    "\n",
    "print(\"Main prediction function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c044b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Running the Prediction\n",
    "# ---------------------------\n",
    "\n",
    "# In a notebook, we don't use argparse. We define the paths directly.\n",
    "# NOTE: You MUST create these dummy files or point to your actual data for the code to run.\n",
    "\n",
    "# 1. Create a dummy folder structure\n",
    "os.makedirs(\"./data\", exist_ok=True)\n",
    "os.makedirs(\"./ref_data/models\", exist_ok=True)\n",
    "os.makedirs(\"./output\", exist_ok=True)\n",
    "\n",
    "\n",
    "# 2. Create a dummy input CSV for journeys\n",
    "dummy_input_data = {\n",
    "    'jid': [101],\n",
    "    'path_to_the_parquet_file': ['./data/dummy_bus_pings.parquet'],\n",
    "    'auto_1_ride_start_location': ['(12.9716, 77.5946)'], # Bangalore center\n",
    "    'auto_1_ride_request_time': ['09:00:00'],\n",
    "    'auto_1_ride_end_bus_stop_ID': [3426], # Example Stop ID\n",
    "    'bus_trip_end_bus_stop_ID': [3430], # Example Stop ID\n",
    "    'auto_2_ride_end_location': ['(12.9791, 77.5913)'] # Near Cubbon Park\n",
    "}\n",
    "pd.DataFrame(dummy_input_data).to_csv(\"./data/input.csv\", index=False)\n",
    "\n",
    "\n",
    "# 3. Create a dummy bus pings parquet file\n",
    "# This data needs to exist for the code to run.\n",
    "dummy_bus_pings = {\n",
    "    'vehicle_timestamp': [1728451200], # Corresponds to 2025-10-09 09:00:00 UTC\n",
    "    'latitude': [12.9720],\n",
    "    'longitude': [77.5950],\n",
    "    'route_id': ['248U'],\n",
    "    'trip_id': ['trip_1']\n",
    "}\n",
    "pd.DataFrame(dummy_bus_pings).to_parquet(\"./data/dummy_bus_pings.parquet\")\n",
    "\n",
    "\n",
    "# 4. Define file paths\n",
    "input_file = \"./data/input.csv\"\n",
    "output_file = \"./output/predictions.json\"\n",
    "\n",
    "\n",
    "# 5. Run the main function\n",
    "# This will likely fail unless you provide the actual reference data files\n",
    "# (models, encoders, cleaned stops, etc.) in the 'ref_data' directory.\n",
    "try:\n",
    "    main(input_csv_path=input_file, output_json_path=output_file)\n",
    "except FileNotFoundError as e:\n",
    "    print(\"\\n-------------------------------------------------------------\")\n",
    "    print(f\"EXECUTION FAILED: {e}\")\n",
    "    print(\"This is expected if you haven't provided the required reference data files in the './ref_data' directory.\")\n",
    "    print(\"Please ensure the following files exist:\")\n",
    "    print(f\" - {STOPS_PATH}\")\n",
    "    print(f\" - {ROUTE_SEQ_PATH}\")\n",
    "    print(f\" - {MODEL_PATH}\")\n",
    "    print(f\" - {ENCODERS_PATH}\")\n",
    "    print(f\" - {SPEED_PATH}\")\n",
    "    print(f\" - {SDR_PATH}\")\n",
    "    print(f\" - {AUTO_SPEED_PATH}\")\n",
    "    print(\"-------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1828e8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
