{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "719f3b80",
   "metadata": {},
   "source": [
    "# ETA Prediction Pipeline Report\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook implements a pipeline to predict the Expected Time of Arrival (ETA) for ride requests. The ETA is decomposed into three key components, each predicted by a separate model:\n",
    "\n",
    "1.  **$P_a$ (Driver Search Time):** The estimated time it takes for a driver to accept the ride request. This is heavily influenced by the local supply-to-demand ratio (SDR).\n",
    "2.  **$P_b$ (Driver-to-Pickup Time):** The estimated time for the assigned driver to travel to the user's pickup location. This is also influenced by the local SDR.\n",
    "3.  **$P_c$ (Trip Duration):** The estimated time for the actual ride from the pickup point to the destination. This is calculated using road network data and historical average speeds.\n",
    "\n",
    "The pipeline processes a CSV of ride requests, enriches it with geospatial data (SDR and road speeds), applies the models, and outputs the predicted $P_a, P_b,$ and $P_c$ values in a JSON file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eb00bf",
   "metadata": {},
   "source": [
    "### 1. Setup and Imports\n",
    "\n",
    "First, we import all the necessary libraries. This includes `pandas` for data manipulation, `h3` for hexagonal geospatial indexing, `osmnx` and `networkx` for road network analysis, and others for various utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f909a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import h3\n",
    "import ast\n",
    "import networkx as nx\n",
    "import osmnx as ox\n",
    "from shapely.geometry import Point, Polygon, box\n",
    "from shapely.ops import transform\n",
    "from pyproj import CRS, Transformer\n",
    "\n",
    "# Set pandas display options for better viewing\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cba768b",
   "metadata": {},
   "source": [
    "### 2. Configuration and Model Parameters\n",
    "\n",
    "These parameters control the behavior of the models and the data processing steps. They are centralized here for easy tuning and experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7d4304c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- General Configuration ---\n",
    "# Set to True to use a constant 10 minutes for pc, or False to calculate it dynamically.\n",
    "USE_CONSTANT_PC = False \n",
    "\n",
    "# --- Heuristic Model Parameters ---\n",
    "# H3 is a geospatial indexing system that divides the world into hexagons. Resolution 7 is a good balance for city-level analysis.\n",
    "H3_RES = 7\n",
    "# The radius around a request point to consider for local supply/demand.\n",
    "CIRCLE_RADIUS_M = 1840.0\n",
    "\n",
    "# --- Pa Model Parameters (Driver Search Time) ---\n",
    "# Best parameters found from testing for the Pa logistic function\n",
    "PA_ALPHA = 2.0\n",
    "PA_BETA = 0.0\n",
    "\n",
    "# --- Pb Model Parameters (Driver-to-Pickup Time) ---\n",
    "# Parameters for the Pb exponential model\n",
    "PB_ALPHA = 0.2\n",
    "PB_BETA = 0\n",
    "\n",
    "# --- Pc Model Parameters (Trip Duration) ---\n",
    "# Fallback constant for trip time (pc) in minutes\n",
    "PC_CONST = 10.0\n",
    "\n",
    "# --- Logic Thresholds ---\n",
    "# SDR value below which a ride is considered a \"failed attempt\" with high Pa\n",
    "SDR_FAILURE_THRESHOLD = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3c9ed9",
   "metadata": {},
   "source": [
    "### 3. File Paths\n",
    "\n",
    "In a notebook environment, we define the input and output file paths directly, replacing the command-line argument parser from the original script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c32d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for local execution\n",
    "DEFAULT_INPUT_CSV = \"data/input.csv\"\n",
    "DEFAULT_OUTPUT_JSON = \"out/output.json\"\n",
    "DEFAULT_SDR_PATH = \"ref_data/processed_hex_avg_SDR.parquet\"\n",
    "DEFAULT_SPEED_PATH = \"ref_data/smoothed_speed_full.parquet\"\n",
    "\n",
    "# Check if required directories exist, create if not\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "os.makedirs(\"out\", exist_ok=True)\n",
    "os.makedirs(\"ref_data\", exist_ok=True)\n",
    "\n",
    "print(\"Please ensure the following files are placed correctly:\")\n",
    "print(f\"Input ride data: '{DEFAULT_INPUT_CSV}'\")\n",
    "print(f\"SDR reference data: '{DEFAULT_SDR_PATH}'\")\n",
    "print(f\"Speed reference data: '{DEFAULT_SPEED_PATH}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f2c085",
   "metadata": {},
   "source": [
    "### 4. Utility Functions\n",
    "\n",
    "This section contains the helper functions used throughout the pipeline for tasks like time conversion, coordinate parsing, and geospatial calculations.\n",
    "\n",
    "#### 4.1. Data Parsing and Conversion\n",
    "\n",
    "Functions to process raw data from the input files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d49790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_slot(timestr):\n",
    "    \"\"\"Converts a 'HH:MM:SS' string to a 15-minute time slot index (0-95).\"\"\"\n",
    "    try:\n",
    "        hh, mm, ss = [int(x) for x in timestr.split(\":\")]\n",
    "    except Exception:\n",
    "        return 0\n",
    "    total_minutes = hh * 60 + mm\n",
    "    return min(max(total_minutes // 15, 0), 95)\n",
    "\n",
    "def parse_coordinates(s):\n",
    "    \"\"\"Parses a string like '(lat, lon)' into a tuple of floats.\"\"\"\n",
    "    if pd.isna(s):\n",
    "        return (None, None)\n",
    "    # Strip parentheses and quotes before splitting\n",
    "    ss = str(s).strip().replace(\"(\", \"\").replace(\")\", \"\").replace('\"', '')\n",
    "    parts = [p.strip() for p in ss.split(\",\")]\n",
    "    \n",
    "    if len(parts) != 2:\n",
    "        return (None, None)\n",
    "    try:\n",
    "        # Latitude is first, Longitude is second\n",
    "        return float(parts[0]), float(parts[1])\n",
    "    except Exception:\n",
    "        return (None, None)\n",
    "\n",
    "def convert_h3_to_int64(h3_hex_str):\n",
    "    \"\"\"Converts a hexadecimal H3 string to a 64-bit integer, safely handling invalid inputs.\"\"\"\n",
    "    if pd.isna(h3_hex_str) or h3_hex_str is None:\n",
    "        return np.int64(0)\n",
    "    try:\n",
    "        return np.int64(int(str(h3_hex_str), 16))\n",
    "    except ValueError:\n",
    "        return np.int64(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39669628",
   "metadata": {},
   "source": [
    "#### 4.2. Geospatial and H3 Functions\n",
    "\n",
    "These functions handle the core geospatial logic. We use an Azimuthal Equidistant (AEQD) projection to accurately calculate areas and distances in meters. The `compute_hex_circle_weights` function is crucial: it finds all H3 hexagons that overlap with a circle of a given radius around the ride request's origin and calculates the percentage of overlap. This weighted area is used to calculate a weighted average SDR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd40369c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_h3_index(lat, lon, res):\n",
    "    \"\"\"Finds the H3 hexagon index for a given lat/lon and resolution.\"\"\"\n",
    "    return h3.latlng_to_cell(lat, lon, res)\n",
    "\n",
    "def build_aeqd_transformer(lat_center, lon_center):\n",
    "    \"\"\"Creates projection transformers for accurate distance calculations.\"\"\"\n",
    "    aeqd_proj4 = f\"+proj=aeqd +lat_0={lat_center} +lon_0={lon_center} +units=m +datum=WGS84 +no_defs\"\n",
    "    aeqd_crs = CRS.from_proj4(aeqd_proj4)\n",
    "    wgs84 = CRS.from_epsg(4326)\n",
    "    to_proj = Transformer.from_crs(wgs84, aeqd_crs, always_xy=True)\n",
    "    to_wgs84 = Transformer.from_crs(aeqd_crs, wgs84, always_xy=True)\n",
    "    return to_proj.transform, to_wgs84.transform\n",
    "\n",
    "def compute_hex_circle_weights(lat, lon, radius_m=CIRCLE_RADIUS_M, h3_res=H3_RES, k_ring_k=3):\n",
    "    \"\"\"\n",
    "    Calculates the proportional overlap of H3 hexagons within a specified radius\n",
    "    of a central point. This is used to create a weighted SDR average.\n",
    "    \"\"\"\n",
    "    center_h3_hexadecimal = get_h3_index(lat, lon, h3_res)\n",
    "    if not center_h3_hexadecimal:\n",
    "        return []\n",
    "        \n",
    "    # Get candidate hexagons in the vicinity\n",
    "    candidates = h3.grid_disk(center_h3_hexadecimal, k_ring_k)\n",
    "\n",
    "    # Project circle to a coordinate system where distance is accurate (in meters)\n",
    "    to_proj, _ = build_aeqd_transformer(lat, lon)\n",
    "    cx, cy = to_proj(lon, lat)\n",
    "    circle_proj = Point(cx, cy).buffer(radius_m, resolution=64)\n",
    "\n",
    "    weights = []\n",
    "    for h_hex_str in candidates: \n",
    "        h3_index_int64 = convert_h3_to_int64(h_hex_str)\n",
    "        hex_coords = [(lng, lat) for lat, lng in h3.cell_to_boundary(h_hex_str)]\n",
    "        hex_poly_wgs = Polygon(hex_coords)\n",
    "        hex_poly_proj = transform(lambda x, y: to_proj(x, y), hex_poly_wgs)\n",
    "\n",
    "        # Calculate the intersection area and the corresponding weight\n",
    "        inter = hex_poly_proj.intersection(circle_proj)\n",
    "        inter_area = inter.area if not inter.is_empty else 0.0\n",
    "        hex_area = hex_poly_proj.area if hex_poly_proj.area > 0 else 1.0\n",
    "        weight = inter_area / hex_area\n",
    "\n",
    "        if weight > 0:\n",
    "            weights.append((h3_index_int64, weight))\n",
    "            \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5ac8ae",
   "metadata": {},
   "source": [
    "### 5. ETA Component Models\n",
    "\n",
    "These are the core predictive models for each component of the ETA.\n",
    "\n",
    "#### 5.1. $P_a$ and $P_b$ Heuristic Models\n",
    "\n",
    "$P_a$ (driver search time) and $P_b$ (driver-to-pickup time) are predicted using heuristic models based on the weighted average Supply-to-Demand Ratio (SDR) in the area of the ride request. The logic is that higher SDR (more drivers than riders) leads to shorter wait times.\n",
    "\n",
    "The $P_a$ model uses a logistic function. The probability $p$ is calculated as:\n",
    "$$p = \\frac{1}{1 + e^{-(\\alpha \\cdot \\log(1+y) + \\beta)}}$$\n",
    "Where $y$ is the weighted SDR. The final prediction is $P_a = 9 \\cdot (1 - p)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e3ca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_sdr(sdr_series, h3_index, slot, fallback=0.0):\n",
    "    \"\"\"Efficiently looks up the SDR value from a pre-processed series.\"\"\"\n",
    "    try:\n",
    "        return float(sdr_series.loc[(h3_index, slot)])\n",
    "    except KeyError:\n",
    "        return fallback\n",
    "\n",
    "def predict_pa_robust(SDR_list, area_list, alpha=PA_ALPHA, beta=PA_BETA):\n",
    "    \"\"\"Predicts Pa (driver search time) based on weighted SDR.\"\"\"\n",
    "    SDR_list = np.array(SDR_list)\n",
    "    area_list = np.array(area_list)\n",
    "    weighted_sdr = np.sum(SDR_list * area_list)\n",
    "    \n",
    "    # If SDR is very low, assume a failed attempt and return a high wait time\n",
    "    if weighted_sdr < SDR_FAILURE_THRESHOLD:\n",
    "        return 30.0\n",
    "    else:\n",
    "        log_transformed_sdr = math.log(1 + weighted_sdr)\n",
    "        prob = 1 / (1 + np.exp(-(alpha * log_transformed_sdr + beta)))\n",
    "        predicted_pa = 9.0 * (1 - prob)\n",
    "        return predicted_pa\n",
    "\n",
    "def predict_pb_exponential(SDR_list, area_list, alpha=PB_ALPHA, beta=PB_BETA):\n",
    "    \"\"\"Predicts Pb (driver-to-pickup time) based on weighted SDR.\"\"\"\n",
    "    SDR_list = np.array(SDR_list)\n",
    "    area_list = np.array(area_list)\n",
    "    y = np.sum(SDR_list * area_list)\n",
    "    prob = 1 / (1 + np.exp(-(alpha * y + beta)))\n",
    "    return 7 * (1 - prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee38470d",
   "metadata": {},
   "source": [
    "#### 5.2. $P_c$ Dynamic Trip Time Model\n",
    "\n",
    "The trip time, $P_c$, is calculated by finding the shortest path on the actual road network using `OSMnx`. The travel time for each road segment in the path is estimated using a pre-computed lookup table of historical average speeds for that segment and time of day. This provides a much more accurate estimate than simple straight-line distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec63f850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_speed_in_df(u, v, speed_df, timeslot_col, G):\n",
    "    \"\"\"Looks up the average speed for a given road segment (u, v) at a specific time.\"\"\"\n",
    "    def mean_speed(df):\n",
    "        return float(df[timeslot_col].mean()) if not df.empty else None\n",
    "\n",
    "    # Try to find an exact match for the edge\n",
    "    match = speed_df[((speed_df['u'] == u) & (speed_df['v'] == v)) | ((speed_df['u'] == v) & (speed_df['v'] == u))]\n",
    "    if not match.empty: return mean_speed(match)\n",
    "\n",
    "    # Fallback to matching by OpenStreetMap ID\n",
    "    edge_data_dict = G.get_edge_data(u, v)\n",
    "    osmid_set = set()\n",
    "    if edge_data_dict:\n",
    "        for _, attr in edge_data_dict.items():\n",
    "            osmid = attr.get('osmid')\n",
    "            if isinstance(osmid, list): osmid_set.update(osmid)\n",
    "            elif osmid is not None: osmid_set.add(osmid)\n",
    "    if osmid_set:\n",
    "        match = speed_df[speed_df['osmid'].isin(osmid_set)]\n",
    "        if not match.empty: return mean_speed(match)\n",
    "        \n",
    "    return np.nan\n",
    "\n",
    "def predict_pc_dynamic_graph(start_lat, start_lon, end_lat, end_lon, time_str, speed_df, default_speed_kph=19.5):\n",
    "    \"\"\"\n",
    "    Calculates route travel time by downloading the street network for the ride's\n",
    "    bounding box, finding the shortest path, and summing segment travel times.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Determine the time slot for speed lookup\n",
    "        hh, mm, ss = [int(x) for x in time_str.split(\":\")]\n",
    "        seconds_from_midnight = hh * 3600 + mm * 60 + ss\n",
    "        timeslot = str(seconds_from_midnight // 900)\n",
    "\n",
    "        # Download the relevant section of the map\n",
    "        buffer_deg = 0.01\n",
    "        north, south = max(start_lat, end_lat) + buffer_deg, min(start_lat, end_lat) - buffer_deg\n",
    "        east, west = max(start_lon, end_lon) + buffer_deg, min(start_lon, end_lon) - buffer_deg\n",
    "        G = ox.graph_from_bbox(north, south, east, west, network_type=\"drive_service\", simplify=True)\n",
    "\n",
    "        # Find the nearest network nodes to the start/end points and calculate the route\n",
    "        start_node, end_node = ox.nearest_nodes(G, X=(start_lon, end_lon), Y=(start_lat, end_lat))\n",
    "        route_nodes = nx.shortest_path(G, source=start_node, target=end_node, weight='length')\n",
    "        \n",
    "        total_time_sec = 0\n",
    "        last_speed = default_speed_kph\n",
    "        \n",
    "        # Iterate over segments in the route to calculate total time\n",
    "        for i in range(len(route_nodes) - 1):\n",
    "            u, v = route_nodes[i], route_nodes[i+1]\n",
    "            avg_speed = edge_speed_in_df(u, v, speed_df, timeslot, G)\n",
    "            \n",
    "            if pd.isna(avg_speed) or avg_speed < 0.5:\n",
    "                avg_speed = last_speed\n",
    "            \n",
    "            last_speed = avg_speed\n",
    "            edge_length_m = G.get_edge_data(u, v, key=0).get('length', 0)\n",
    "            \n",
    "            if avg_speed > 0:\n",
    "                time_sec = (edge_length_m / 1000) / avg_speed * 3600\n",
    "                total_time_sec += time_sec\n",
    "                \n",
    "        return total_time_sec / 60\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"PC WARNING: Dynamic graph calculation failed (Error: {e}). Using fallback PC={PC_CONST}.\")\n",
    "        return PC_CONST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf5ea5f",
   "metadata": {},
   "source": [
    "### 6. Main Execution Pipeline\n",
    "\n",
    "This is the main part of the notebook where we load the data, process each ride request one by one, and generate the final predictions.\n",
    "\n",
    "#### 6.1. Load and Prepare Data\n",
    "\n",
    "We load the input CSV with ride requests and the Parquet files containing the SDR and speed data. The SDR data is then \"melted\" from a wide format (one column per time slot) to a long format, which allows for much faster lookups using a multi-index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b32f575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Data ---\n",
    "try:\n",
    "    df_rides = pd.read_csv(DEFAULT_INPUT_CSV, dtype=str)\n",
    "    print(f\"SUCCESS: Input CSV ({len(df_rides)} rows) loaded.\")\n",
    "    display(df_rides.head())\n",
    "    \n",
    "    sdr_df = pd.read_parquet(DEFAULT_SDR_PATH)\n",
    "    print(f\"SUCCESS: SDR data ({len(sdr_df)} hexes) loaded.\")\n",
    "    display(sdr_df.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"FATAL ERROR: Could not load core data files. Please check paths. Error: {e}\")\n",
    "\n",
    "# --- Pre-process SDR Data for efficient lookup ---\n",
    "slot_columns = [f'slot_{i}' for i in range(96)]\n",
    "sdr_long_df = sdr_df.melt(id_vars=['h3_index'], value_vars=slot_columns, var_name='slot_name', value_name='SDR')\n",
    "sdr_long_df['slot'] = sdr_long_df['slot_name'].str.split('_').str[-1].astype(int)\n",
    "sdr_long_df['h3_index'] = sdr_long_df['h3_index'].astype(np.int64)\n",
    "sdr_series = sdr_long_df.set_index([\"h3_index\", \"slot\"])[\"SDR\"]\n",
    "print(f\"SUCCESS: SDR data prepared for lookup.\")\n",
    "\n",
    "# --- Load Speed Data ---\n",
    "speed_df = None\n",
    "if os.path.exists(DEFAULT_SPEED_PATH):\n",
    "    try:\n",
    "        speed_df = pd.read_parquet(DEFAULT_SPEED_PATH)\n",
    "        if 'u' in speed_df.columns: speed_df['u'] = speed_df['u'].astype(np.int64)\n",
    "        if 'v' in speed_df.columns: speed_df['v'] = speed_df['v'].astype(np.int64)\n",
    "        print(f\"SUCCESS: Speed Data loaded from '{DEFAULT_SPEED_PATH}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: Speed Data load FAILED: {e}. PC will use fallback.\")\n",
    "else:\n",
    "    print(f\"WARNING: Speed Data file NOT FOUND. PC will use fallback.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a56cfeb",
   "metadata": {},
   "source": [
    "#### 6.2. Process Each Ride\n",
    "\n",
    "We iterate through each row of the input DataFrame, apply the functions and models defined above, and store the results in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b35b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "print(\"\\nStarting prediction for each ride...\")\n",
    "\n",
    "for index, row in df_rides.iterrows():\n",
    "    rid = row[\"rid\"]\n",
    "    print(f\"--- Processing rid: {rid} ---\")\n",
    "    \n",
    "    # 1. Parse inputs\n",
    "    slot = time_to_slot(row[\"ride_request_time\"])\n",
    "    lat, lon = parse_coordinates(row[\"ride_start_location\"])\n",
    "    end_lat, end_lon = parse_coordinates(row[\"ride_end_location\"])\n",
    "\n",
    "    if lat is None or lon is None or end_lat is None or end_lon is None:\n",
    "        results[rid] = {\"pa\": 0.0, \"pb\": 0.0, \"pc\": PC_CONST}\n",
    "        print(f\"  -> Invalid coordinates. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # 2. Get weighted SDR for Pa and Pb models\n",
    "    weights = compute_hex_circle_weights(lat, lon)\n",
    "    if not weights:\n",
    "        # Fallback to just the center hex if circle calculation fails\n",
    "        center_h3_hex = get_h3_index(lat, lon, H3_RES)\n",
    "        center_h3_int = convert_h3_to_int64(center_h3_hex)\n",
    "        if center_h3_int != np.int64(0):\n",
    "            weights = [(center_h3_int, 1.0)]\n",
    "        else:\n",
    "            results[rid] = {\"pa\": 0.0, \"pb\": 0.0, \"pc\": PC_CONST}\n",
    "            print(f\"  -> Invalid H3 index. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "    SDRs, areas = [], []\n",
    "    for h, w in weights:\n",
    "        SDRs.append(lookup_sdr(sdr_series, h, slot))\n",
    "        areas.append(w)\n",
    "\n",
    "    # 3. Predict Pa and Pb\n",
    "    pa = predict_pa_robust(SDRs, areas)\n",
    "    pb = predict_pb_exponential(SDRs, areas)\n",
    "    print(f\"  -> Pa={pa:.2f}, Pb={pb:.2f}\")\n",
    "\n",
    "    # 4. Predict Pc\n",
    "    if USE_CONSTANT_PC:\n",
    "        pc = PC_CONST\n",
    "    elif speed_df is not None:\n",
    "        pc = predict_pc_dynamic_graph(lat, lon, end_lat, end_lon, row[\"ride_request_time\"], speed_df)\n",
    "    else:\n",
    "        pc = PC_CONST # Fallback if speed data is missing\n",
    "    print(f\"  -> Pc={pc:.2f}\")\n",
    "\n",
    "    # 5. Store results\n",
    "    results[rid] = {\"pa\": round(pa, 2), \"pb\": round(pb, 2), \"pc\": round(pc, 2)}\n",
    "    \n",
    "print(\"\\n...Processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba29940",
   "metadata": {},
   "source": [
    "### 7. Save and Display Results\n",
    "\n",
    "Finally, we save the `results` dictionary to the specified output JSON file and also display the first few results as a DataFrame for a quick review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fa2444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to a JSON file\n",
    "with open(DEFAULT_OUTPUT_JSON, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\nSUCCESS: Results saved to {DEFAULT_OUTPUT_JSON}\")\n",
    "\n",
    "# Display results in a DataFrame for review\n",
    "results_df = pd.DataFrame.from_dict(results, orient='index')\n",
    "print(\"\\n--- Final Predictions ---\")\n",
    "display(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_approach",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
