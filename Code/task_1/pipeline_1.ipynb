{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETA Prediction Pipeline Report\n",
    "\n",
    "This notebook documents and executes a pipeline to predict the Estimated Time of Arrival (ETA) for vehicles along predefined routes. The process involves:\n",
    "\n",
    "1.  **Loading Core Assets**: This includes reference data like stop locations, route sequences, and pre-trained machine learning models.\n",
    "2.  **Defining Helper Functions**: A set of utility functions for data cleaning, parsing, and calculations.\n",
    "3.  **Executing the Prediction Logic**: The main part of the pipeline that processes input vehicle pings (from Parquet files) and generates ETA predictions for future stops.\n",
    "4.  **Saving Results**: The final predictions are saved to a JSON file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import pickle\n",
    "from datetime import timedelta\n",
    "from geopy.distance import geodesic\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"xgboost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions\n",
    "\n",
    "This section contains all the utility functions required for the pipeline. They handle tasks like cleaning strings, parsing timestamps, resolving file paths in different environments, and calculating the average speed of a trip from raw GPS data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_quoted(x):\n",
    "    if pd.isna(x):\n",
    "        return x\n",
    "    return str(x).strip().strip('\"').strip(\"'\")\n",
    "\n",
    "def parse_vehicle_timestamp(ts_raw):\n",
    "    if ts_raw is None:\n",
    "        return None\n",
    "    s = clean_quoted(ts_raw)\n",
    "    if s == \"\":\n",
    "        return None\n",
    "    ts = None\n",
    "    if re.fullmatch(r\"\\d+\", s):\n",
    "        try:\n",
    "            ts = pd.to_datetime(int(s), unit=\"s\", utc=True)  # keep UTC\n",
    "        except Exception:\n",
    "            ts = pd.to_datetime(s, errors=\"coerce\", utc=True)\n",
    "    else:\n",
    "        ts = pd.to_datetime(s, errors=\"coerce\", utc=True)\n",
    "    return ts\n",
    "\n",
    "def parse_stop_list(stop_list_str):\n",
    "    if pd.isna(stop_list_str):\n",
    "        return []\n",
    "    return [int(n) for n in re.findall(r\"\\d+\", str(stop_list_str))]\n",
    "\n",
    "def resolve_path(docker_path, local_path):\n",
    "    return docker_path if os.path.exists(docker_path) else local_path\n",
    "\n",
    "def safe_read_parquet(parquet_path):\n",
    "    if os.path.exists(parquet_path):\n",
    "        return pd.read_parquet(parquet_path)\n",
    "    alt_path = os.path.join(\"/app/data\", parquet_path)\n",
    "    if os.path.exists(alt_path):\n",
    "        print(f\"[INFO] Using alternate path: {alt_path}\")\n",
    "        return pd.read_parquet(alt_path)\n",
    "    base_name = os.path.basename(parquet_path)\n",
    "    alt_eval_path = os.path.join(\"/app/data/eval_data\", base_name)\n",
    "    if os.path.exists(alt_eval_path):\n",
    "        print(f\"[INFO] Using alternate eval_data path: {alt_eval_path}\")\n",
    "        return pd.read_parquet(alt_eval_path)\n",
    "    raise FileNotFoundError(f\"Parquet file not found at: {parquet_path}, checked also {alt_path} and {alt_eval_path}\")\n",
    "\n",
    "def compute_trip_avg_speed(df):\n",
    "    \"\"\"\n",
    "    Compute average speed (m/s) for a trip DataFrame with lat/lon + vehicle_timestamp.\n",
    "    \"\"\"\n",
    "    if df.shape[0] < 2:\n",
    "        return None  # not enough points\n",
    "\n",
    "    df = df.sort_values(\"vehicle_timestamp\").copy()\n",
    "    df[\"vehicle_timestamp\"] = pd.to_datetime(df[\"vehicle_timestamp\"], errors=\"coerce\", utc=True)\n",
    "\n",
    "    total_dist = 0.0\n",
    "    total_time = 0.0\n",
    "\n",
    "    prev_lat, prev_lon, prev_time = None, None, None\n",
    "    for _, row in df.iterrows():\n",
    "        lat, lon, t = row.get(\"latitude\"), row.get(\"longitude\"), row[\"vehicle_timestamp\"]\n",
    "        if pd.isna(lat) or pd.isna(lon) or pd.isna(t):\n",
    "            continue\n",
    "        if prev_lat is not None and prev_time is not None:\n",
    "            dist = geodesic((prev_lat, prev_lon), (lat, lon)).meters\n",
    "            dt = (t - prev_time).total_seconds()\n",
    "            if dt > 0 and dist > 0:\n",
    "                total_dist += dist\n",
    "                total_time += dt\n",
    "        prev_lat, prev_lon, prev_time = lat, lon, t\n",
    "\n",
    "    if total_time > 0:\n",
    "        return total_dist / total_time  # m/s\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading Core Assets\n",
    "\n",
    "Here, we load all the necessary reference data and pre-trained models. This includes:\n",
    "- `stops_clean.csv`: A CSV file containing the geographic coordinates for each stop ID.\n",
    "- `route_to_stop_clean.csv`: A mapping of each route to its ordered sequence of stop IDs.\n",
    "- `eta_model_hypertuned.pkl`: The pre-trained XGBoost regression model for predicting travel time between stops.\n",
    "- `encoders_hypertuned.pkl`: The label encoders for categorical features (like `route_id`, `from_stop`).\n",
    "- `route_avg_speed_m_s_hypertuned.pkl`: Pre-computed average speeds for each route, used as a fallback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to reference data and models\n",
    "STOPS_PATH = resolve_path(\"/app/refdata/stops_clean.csv\", \"../refdata/stops_clean.csv\")\n",
    "ROUTE_SEQ_PATH = resolve_path(\"/app/refdata/route_to_stop_clean.csv\", \"../refdata/route_to_stop_clean.csv\")\n",
    "MODEL_PATH = resolve_path(\"/app/refdata/models/eta_model_hypertuned.pkl\", \"../refdata/models/eta_model_hypertuned.pkl\")\n",
    "ENCODERS_PATH = resolve_path(\"/app/refdata/models/encoders_hypertuned.pkl\", \"../refdata/models/encoders_hypertuned.pkl\")\n",
    "SPEED_PATH = resolve_path(\"/app/refdata/models/route_avg_speed_m_s_hypertuned.pkl\", \"../refdata/models/route_avg_speed_m_s_hypertuned.pkl\")\n",
    "\n",
    "# Load dataframes\n",
    "df_stops = pd.read_csv(STOPS_PATH)\n",
    "df_route_seq = pd.read_csv(ROUTE_SEQ_PATH, index_col=0)\n",
    "df_route_seq['route_id'] = df_route_seq['route_id'].astype(str).str.strip()\n",
    "\n",
    "if 'stop_id' in df_stops.columns:\n",
    "    try:\n",
    "        df_stops['stop_id'] = df_stops['stop_id'].astype(int)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Load model and encoders\n",
    "model = joblib.load(MODEL_PATH)\n",
    "with open(ENCODERS_PATH, \"rb\") as f:\n",
    "    encoders = pickle.load(f)\n",
    "\n",
    "# Load pre-computed average speeds\n",
    "route_avg_speed_m_s = joblib.load(SPEED_PATH)\n",
    "global_avg_speed_m_s = float(pd.Series(route_avg_speed_m_s).mean())\n",
    "\n",
    "print(\"✅ Core assets loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Core Prediction Functions\n",
    "\n",
    "These two functions form the core of the prediction engine.\n",
    "- `predict_future_stops`: Takes a vehicle's latest ping (location and time), its route, and its average speed to predict the arrival time at all subsequent stops on its route.\n",
    "- `apply_floor_limit`: A business logic function that ensures a predicted ETA is not in the past or too close to the current time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_floor_limit(eta_time, ping_time, min_gap_minutes=0):\n",
    "    if eta_time <= ping_time + timedelta(minutes=min_gap_minutes):\n",
    "        eta_time = ping_time + timedelta(minutes=min_gap_minutes)\n",
    "    return eta_time\n",
    "\n",
    "def predict_future_stops(ping, route_id, stop_seq, avg_speed):\n",
    "    ts = parse_vehicle_timestamp(ping.get('vehicle_timestamp', None))\n",
    "    if ts is pd.NaT or ts is None:\n",
    "        raise ValueError(\"Cannot parse vehicle timestamp for ping.\")\n",
    "    ping_time = ts  # UTC\n",
    "\n",
    "    coords = df_stops.set_index('stop_id').loc[stop_seq][['stop_lat', 'stop_lon']].reset_index()\n",
    "    dists = coords.apply(lambda r: geodesic((float(ping['ping_lat']), float(ping['ping_lon'])),\n",
    "                                            (float(r.stop_lat), float(r.stop_lon))).meters, axis=1).values\n",
    "    next_idx = int(np.argmin(dists))\n",
    "    etas = {}\n",
    "    cumulative = 0.0\n",
    "\n",
    "    to_stop_id = int(coords.loc[next_idx, 'stop_id'])\n",
    "    remaining_distance_m = geodesic((float(ping['ping_lat']), float(ping['ping_lon'])),\n",
    "                                      (float(coords.loc[next_idx, 'stop_lat']), float(coords.loc[next_idx, 'stop_lon']))).meters\n",
    "    from_stop_id = int(stop_seq[next_idx - 1]) if next_idx > 0 else stop_seq[next_idx]\n",
    "\n",
    "    try: r_enc = encoders['route_id'].transform([str(route_id)])[0] if str(route_id) in encoders['route_id'].classes_ else 0\n",
    "    except: r_enc = 0\n",
    "    try: fs_enc = encoders['from_stop'].transform([str(from_stop_id)])[0] if str(from_stop_id) in encoders['from_stop'].classes_ else 0\n",
    "    except: fs_enc = 0\n",
    "    try: ts_enc = encoders['to_stop'].transform([str(to_stop_id)])[0] if str(to_stop_id) in encoders['to_stop'].classes_ else 0\n",
    "    except: ts_enc = 0\n",
    "\n",
    "    # Predict time to the very next stop\n",
    "    first_X = pd.DataFrame([{\n",
    "        'route_id_enc': r_enc,\n",
    "        'from_stop_enc': fs_enc,\n",
    "        'to_stop_enc': ts_enc,\n",
    "        'distance_m': remaining_distance_m,\n",
    "        'avg_speed_m_s': avg_speed,\n",
    "        'start_hour': int(ping_time.hour),\n",
    "        'day_of_week': int(ping_time.dayofweek)\n",
    "    }])\n",
    "    cumulative += float(np.expm1(model.predict(first_X)[0]))\n",
    "    eta_first = ping_time + timedelta(minutes=cumulative)\n",
    "    eta_first = apply_floor_limit(eta_first, ping_time)\n",
    "    etas[to_stop_id] = eta_first.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Predict time for all subsequent stop-to-stop segments\n",
    "    for j in range(next_idx + 1, len(stop_seq)):\n",
    "        f_stop, t_stop = int(stop_seq[j-1]), int(stop_seq[j])\n",
    "        fcoord = df_stops.set_index('stop_id').loc[f_stop]\n",
    "        tcoord = df_stops.set_index('stop_id').loc[t_stop]\n",
    "        segdist = geodesic((float(fcoord.stop_lat), float(fcoord.stop_lon)),\n",
    "                             (float(tcoord.stop_lat), float(tcoord.stop_lon))).meters\n",
    "        try: f_enc = encoders['from_stop'].transform([str(f_stop)])[0] if str(f_stop) in encoders['from_stop'].classes_ else 0\n",
    "        except: f_enc = 0\n",
    "        try: t_enc = encoders['to_stop'].transform([str(t_stop)])[0] if str(t_stop) in encoders['to_stop'].classes_ else 0\n",
    "        except: t_enc = 0\n",
    "        fe = pd.DataFrame([{\n",
    "            'route_id_enc': r_enc,\n",
    "            'from_stop_enc': f_enc,\n",
    "            'to_stop_enc': t_enc,\n",
    "            'distance_m': segdist,\n",
    "            'avg_speed_m_s': avg_speed,\n",
    "            'start_hour': int(ping_time.hour),\n",
    "            'day_of_week': int(ping_time.dayofweek)\n",
    "        }])\n",
    "        cumulative += float(np.expm1(model.predict(fe)[0]))\n",
    "        eta_next = ping_time + timedelta(minutes=cumulative)\n",
    "        eta_next = apply_floor_limit(eta_next, ping_time)\n",
    "        etas[t_stop] = eta_next.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    return etas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pipeline Execution\n",
    "\n",
    "This is the main executable part of the notebook. It replaces the command-line argument handling from the original script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Input Configuration\n",
    "\n",
    "**Modify the `input_path` variable below** to point to your input file. This can be either a single `.parquet` file or a `.json` file that maps team IDs to their respective `.parquet` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURE YOUR INPUT AND OUTPUT PATHS HERE ---\n",
    "input_path = \"/app/data/input.json\" # Example: \"path/to/your/input.json\" or \"path/to/livedata.parquet\"\n",
    "output_path = \"/app/out/output.json\"\n",
    "# -----------------------------------------------------\n",
    "\n",
    "output_dict = {}\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "print(f\"Input path set to: {input_path}\")\n",
    "print(f\"Output path set to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Main Processing Loop\n",
    "\n",
    "The following cell reads the configured input file(s), processes each trip, calculates ETAs, and stores them in the `output_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_path.endswith(\".parquet\"):\n",
    "    print(f\"\\n[INFO] Processing single Parquet file: {input_path}\")\n",
    "    df = safe_read_parquet(input_path)\n",
    "    trip_avg_speed = compute_trip_avg_speed(df)\n",
    "\n",
    "    latest_rows = df.sort_values(\"vehicle_timestamp\").groupby(\"trip_id\").tail(1)\n",
    "    for _, row in latest_rows.iterrows():\n",
    "        if 'latitude' in row and 'longitude' in row:\n",
    "            row['ping_lat'], row['ping_lon'] = row['latitude'], row['longitude']\n",
    "        else:\n",
    "            continue\n",
    "        route_id = clean_quoted(row.get(\"route_id\", \"\"))\n",
    "        if not route_id or route_id.lower() == \"nan\":\n",
    "            continue\n",
    "        matched = df_route_seq[df_route_seq['route_id'].astype(str).str.strip() == route_id]\n",
    "        if matched.empty: continue\n",
    "        stop_seq = parse_stop_list(matched['stop_id_list'].iloc[0])\n",
    "        if not stop_seq: continue\n",
    "\n",
    "        avg_speed = trip_avg_speed if trip_avg_speed is not None else route_avg_speed_m_s.get(str(route_id), global_avg_speed_m_s)\n",
    "        \n",
    "\n",
    "        try:\n",
    "            etas = predict_future_stops(row, route_id, stop_seq, avg_speed)\n",
    "            if route_id not in output_dict:\n",
    "                output_dict[route_id] = {}\n",
    "            output_dict[route_id].update(etas)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error predicting {row['trip_id']}: {e}\")\n",
    "\n",
    "else:\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_input = json.load(f)\n",
    "\n",
    "    if isinstance(raw_input, dict) and all(isinstance(v, str) and v.endswith(\".parquet\") for v in raw_input.values()):\n",
    "        for team_id, parquet_path in raw_input.items():\n",
    "            print(f\"\\n[INFO] Processing input for '{team_id}': {parquet_path}\")\n",
    "            try:\n",
    "                df = safe_read_parquet(parquet_path)\n",
    "            except FileNotFoundError as e:\n",
    "                print(f\"[ERROR] {e}\")\n",
    "                continue\n",
    "            if \"trip_id\" not in df.columns or \"vehicle_timestamp\" not in df.columns:\n",
    "                print(f\"⚠️ Skipping {team_id}: missing columns\")\n",
    "                continue\n",
    "\n",
    "            trip_avg_speed = compute_trip_avg_speed(df)\n",
    "            latest_rows = df.sort_values(\"vehicle_timestamp\").groupby(\"trip_id\").tail(1)\n",
    "            for _, row in latest_rows.iterrows():\n",
    "                if 'latitude' in row and 'longitude' in row:\n",
    "                    row['ping_lat'], row['ping_lon'] = row['latitude'], row['longitude']\n",
    "                else: continue\n",
    "                route_id = clean_quoted(row.get(\"route_id\", \"\"))\n",
    "                if not route_id or route_id.lower() == \"nan\": continue\n",
    "                matched = df_route_seq[df_route_seq['route_id'].astype(str).str.strip() == route_id]\n",
    "                if matched.empty: continue\n",
    "                stop_seq = parse_stop_list(matched['stop_id_list'].iloc[0])\n",
    "                if not stop_seq: continue\n",
    "\n",
    "                avg_speed = trip_avg_speed if trip_avg_speed is not None else route_avg_speed_m_s.get(str(route_id), global_avg_speed_m_s)\n",
    "                \n",
    "\n",
    "                try:\n",
    "                    etas = predict_future_stops(row, route_id, stop_seq, avg_speed)\n",
    "                    if route_id not in output_dict:\n",
    "                        output_dict[route_id] = {}\n",
    "                    output_dict[route_id].update(etas)\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Error predicting {row['trip_id']} in {team_id}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results\n",
    "\n",
    "The final step is to save the collected predictions to the specified output JSON file and print the results for immediate review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output_dict, f, ensure_ascii=False)\n",
    "\n",
    "print(\"\\n✅ Predictions:\")\n",
    "print(json.dumps(output_dict, indent=4, ensure_ascii=False))\n",
    "print(f\"\\n✅ Done — wrote {len(output_dict)} routes to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
