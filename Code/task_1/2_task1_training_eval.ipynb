{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58cdde78",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d7a9eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# ETA Prediction Hyperparameter Tuning with XGBoost\n",
    "# =======================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef097bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_df = pd.read_parquet(r\"C:\\Users\\moham\\OneDrive\\Desktop\\Bengaluru Vega\\Code\\task_1\\ref_data\\clean_segments.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba91612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================\n",
    "# ETA Prediction Hyperparameter Tuning with XGBoost\n",
    "# =======================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from xgboost import XGBRegressor\n",
    "import optuna\n",
    "\n",
    "# ----------------------------\n",
    "# Preprocessing function\n",
    "# ----------------------------\n",
    "def preprocess_seg_df(seg_df):\n",
    "    df = seg_df.copy()\n",
    "\n",
    "    # Label encode categorical IDs\n",
    "    encoders = {}\n",
    "    for col in ['route_id', 'from_stop', 'to_stop']:\n",
    "        le = LabelEncoder()\n",
    "        df[f\"{col}_enc\"] = le.fit_transform(df[col].astype(str))\n",
    "        encoders[col] = le\n",
    "\n",
    "    # Ensure start_hour and day_of_week exist as numeric\n",
    "    if 'start_hour' not in df.columns:\n",
    "        df['start_hour'] = pd.to_datetime(df['tA']).dt.hour\n",
    "    if 'day_of_week' not in df.columns:\n",
    "        df['day_of_week'] = pd.to_datetime(df['tA']).dt.dayofweek\n",
    "\n",
    "    # Target\n",
    "    y = df['travel_time_min']\n",
    "\n",
    "    # Features\n",
    "    feature_cols = ['route_id_enc','from_stop_enc','to_stop_enc','distance_m',\n",
    "                    'avg_speed_m_s','start_hour','day_of_week']\n",
    "    X = df[feature_cols]\n",
    "\n",
    "    return X, y, encoders\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Objective function for Optuna\n",
    "# ----------------------------\n",
    "def objective(trial, seg_df):\n",
    "    X, y, _ = preprocess_seg_df(seg_df)\n",
    "\n",
    "    params = {\n",
    "    \"n_estimators\": trial.suggest_int(\"n_estimators\", 800, 1400),        # more estimators for lower LR\n",
    "    \"max_depth\": trial.suggest_int(\"max_depth\", 3, 6),                   # shallow to prevent overfitting\n",
    "    \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.02, log=True),  # lower learning rate\n",
    "    \"subsample\": trial.suggest_float(\"subsample\", 0.7, 0.9),             # moderate subsample\n",
    "    \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.7, 0.9), # slightly lower colsample\n",
    "    \"gamma\": trial.suggest_float(\"gamma\", 0, 2),                         # mild regularization\n",
    "    \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.1, 1.0, log=True),    # stronger L1 regularization\n",
    "    \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1.0, 5.0, log=True),  # stronger L2 regularization\n",
    "    \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 4, 8),     # moderate to avoid overfitting small nodes\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"random_state\": 42,\n",
    "    \"n_jobs\": 14\n",
    "}\n",
    "    model = XGBRegressor(**params)\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    rmses = []\n",
    "\n",
    "    for train_idx, valid_idx in tscv.split(X):\n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "\n",
    "        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n",
    "\n",
    "        preds = model.predict(X_valid)\n",
    "        rmse = np.sqrt(mean_squared_error(y_valid, preds))  # <-- fixed here\n",
    "        rmses.append(rmse)\n",
    "\n",
    "    return np.mean(rmses)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Hyperparameter tuning\n",
    "# ----------------------------\n",
    "def tune_hyperparams(seg_df, n_trials=50):\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(lambda trial: objective(trial, seg_df), n_trials=n_trials)\n",
    "\n",
    "    print(\"\\nBest Trial:\")\n",
    "    print(\"  RMSE:\", study.best_value)\n",
    "    print(\"  Params:\", study.best_params)\n",
    "    return study.best_params\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Train final model\n",
    "# ----------------------------\n",
    "def train_final_model(seg_df, best_params):\n",
    "    X, y, encoders = preprocess_seg_df(seg_df)\n",
    "    model = XGBRegressor(**best_params)\n",
    "    model.fit(X, y)\n",
    "    return model, encoders\n",
    "\n",
    "\n",
    "# =======================================================\n",
    "# Example usage\n",
    "# =======================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # seg_df = pd.read_csv(\"your_cleaned_seg_df.csv\")  # load your dataframe\n",
    "    best_params = tune_hyperparams(seg_df, n_trials=75)\n",
    "    final_model, encoders = train_final_model(seg_df, best_params)\n",
    "    print(\"\\nFinal model trained with best parameters!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98850c0",
   "metadata": {},
   "source": [
    "These were the best weights : Best Trial:\n",
    "  RMSE: 2.990990335020095\n",
    "  Params: {'n_estimators': 843, 'max_depth': 4, 'learning_rate': 0.0070734825829311945, 'subsample': 0.7770865778757821, 'colsample_bytree': 0.8959516946395695, 'gamma': 1.8847572995409152, 'reg_alpha': 0.8692519934006419, 'reg_lambda': 1.005542707759953, 'min_child_weight': 6}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdd13aa",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_approach",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
